{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Reinforcement learning (RL) is an area of machine learning (ML) concerned w how software agents ought to take actions in an environment in order to maximize the notion of cumulative reward.\n",
    "\n",
    "or\n",
    "\n",
    "RL is teaching a software agent how to behave in an evironment by telling it how good it's doing.\n",
    "\n",
    "## Deep Q Learning\n",
    "This approach extends RL by using a deep neural network to predict the actions.\n",
    "\n",
    "### Reward\n",
    "- eat food: +10\n",
    "- game over: -10\n",
    "- else: 0\n",
    "\n",
    "### Action\n",
    "determines our next move\n",
    "- [1,0,0] -> straight (stay in current direction)\n",
    "- [0,1,0] -> right turn (dependent on where snake is currently facing)\n",
    "- [0,0,1] -> left turn (dependent on where snake is currently facing)\n",
    "\n",
    "### State (11 values)\n",
    "    danger straight,\n",
    "    danger right,\n",
    "    danger left,\n",
    "    direction left,\n",
    "    direction right,\n",
    "    direction up,\n",
    "    direction down,\n",
    "    food left,\n",
    "    food right,\n",
    "    food up,\n",
    "    food down\n",
    "Example: Snake is traveling right with the food down and to the right\n",
    "\n",
    "[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1]\n",
    "\n",
    "### Model\n",
    "Input: 11 neurons (State)\n",
    "\n",
    "Output: 3 neurons (Action)\n",
    "\n",
    "### What is (Deep) Q Learning?\n",
    "Q Value = Quality of action\n",
    "\n",
    "0. Init Q Value (= init model)\n",
    "1. Choose action (model.predict(state)) [or random move, especially at the beginning (tradeoff btwn exploration and exploitation)]\n",
    "2. Perform action\n",
    "3. Measure reward\n",
    "4. Update Q value using loss function (+ train model) [repeat step 1]\n",
    "\n",
    "### Loss Function (Bellman Equation)\n",
    "NewQ(s, a) = Q(s, a) + alpha[R(s, a) + gamma * maxQ'(s', a') - Q(s, a)]\n",
    "\n",
    "NewQ(s, a): New Q value for that state and that action\n",
    "\n",
    "Q(s, a): Current Q value\n",
    "\n",
    "alpha: Learning Rate\n",
    "\n",
    "R(s, a): Reward for taking that action at that state\n",
    "\n",
    "gamma: Discount rate\n",
    "\n",
    "maxQ'(s', a'): Maximum expected future reward given the new s' and all possible actions at that new state\n",
    "\n",
    "### Q Update Rule Simplified:\n",
    "Q = model.predict(state_0)\n",
    "\n",
    "Q_new = R + gamma * max(Q(state_1))\n",
    "\n",
    "Loss function: loss = (Q_new - Q)^2 [MSE]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
